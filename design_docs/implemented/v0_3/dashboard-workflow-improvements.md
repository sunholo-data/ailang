# Dashboard Workflow Improvements (v0.3.12)

**Status**: ‚úÖ IMPLEMENTED  
**Date**: 2025-10-17  
**Context**: Session on 2025-10-17 revealed major gaps in documentation and workflow clarity for release evaluations and dashboard updates. This document captures the problems found and solutions implemented.

## Problems Identified

### 1. **No Clear "Release Checklist"**
- User had to fight through manual commands
- Dashboard update workflow unclear
- No single source of truth for "what do I run for a release?"

### 2. **Agent Delegation Not Used**
- I manually ran commands instead of using eval-orchestrator agent
- CLAUDE.md says "ALWAYS use eval-orchestrator" but I didn't follow it
- Need stronger forcing function to delegate

### 3. **Dashboard Update Commands Not Clear**
- `make benchmark-dashboard` exists but I didn't know about it
- Multi-model aggregation picked old versions (confusing behavior)
- Docusaurus cache issues not documented

### 4. **Eval Instructions Too Buried**
- Critical workflow info is scattered across CLAUDE.md lines 526-620
- Need dedicated "RELEASE WORKFLOW" section at top
- Quick reference needs to be more prominent

## Proposed Solutions

### A. Add "RELEASE CHECKLIST" Section to CLAUDE.md

**Location**: After line 145 (right after "CRITICAL PRINCIPLES")

```markdown
## üéØ RELEASE WORKFLOW - READ THIS FIRST!

**When user says "ready to release" or "update dashboard":**

### Step 1: Delegate to eval-orchestrator agent
```bash
# NEVER run commands manually - use the agent!
Task: eval-orchestrator
Prompt: "Run v0.3.X release evaluation and update dashboard"
```

The agent will handle:
- Running baseline with correct models
- Comparing to previous version
- Updating dashboard JSON + markdown
- Generating release report

### Step 2: If agent needs help, use these commands

**Run baseline (3 dev models - cheap & fast)**:
```bash
make eval-baseline EVAL_VERSION=v0.3.12
# Result: eval_results/baselines/v0.3.12/ (126 runs, ~$0.22)
```

**Update website dashboard**:
```bash
# Option 1: Use specific version (RECOMMENDED)
ailang eval-report eval_results/baselines/v0.3.12 v0.3.12 --format=docusaurus > docs/docs/benchmarks/performance.md
ailang eval-report eval_results/baselines/v0.3.12 v0.3.12 --format=json > docs/static/benchmarks/latest.json

# Option 2: Use make target (aggregates across all baselines)
make benchmark-dashboard
# ‚ö†Ô∏è This picks LATEST result PER MODEL from ALL baselines
# May show mixed versions (gpt5 from v0.3.9, claude from v0.3.11, etc.)
```

**Restart Docusaurus dev server**:
```bash
cd docs && npm run clear  # Clear cache if needed
cd docs && npm start       # Restart server
# Visit: http://localhost:3000/ailang/docs/benchmarks/performance
```

### Step 3: Common Issues

**Problem**: Dashboard shows old version (e.g., v0.3.9 instead of v0.3.12)
**Cause**: `make benchmark-dashboard` uses `--multi-model` which aggregates latest PER MODEL
**Solution**: Use `ailang eval-report` with specific baseline directory instead

**Problem**: "Uncaught runtime errors" / webpack chunk errors in browser
**Cause**: Docusaurus build cache stale
**Solution**:
```bash
cd docs && npm run clear
rm -rf docs/.docusaurus docs/build
cd docs && npm start
```

**Problem**: Dashboard JSON shows "null" for aggregates
**Cause**: Used wrong JSON file (performance matrix vs dashboard JSON)
**Solution**: Copy from `eval_results/baselines/v0.3.12/` not `eval_results/performance_tables/`

### Step 4: Verification

Before announcing release:
- [ ] Dashboard at http://localhost:3000/ailang/docs/benchmarks/performance loads without errors
- [ ] Timeline chart shows v0.3.X as latest point
- [ ] Success rate matches expected (e.g., 62.7% for v0.3.12)
- [ ] Model breakdown shows all 3 dev models (or 6 if FULL=true)
- [ ] History preserved (shows v0.3.11, v0.3.10, etc.)

---

**‚ùå WRONG APPROACH** (what I did in this session):
- Run `ailang eval-report` directly without checking make targets
- Try to manually copy/edit JSON files
- Run `make benchmark-dashboard` without understanding multi-model behavior
- Manually fix Docusaurus cache issues

**‚úÖ CORRECT APPROACH**:
- Delegate to eval-orchestrator agent FIRST
- Use `make eval-baseline EVAL_VERSION=vX.Y.Z` for storage
- Use `ailang eval-report <specific_dir> vX.Y.Z` for dashboard
- Let make targets handle the complexity
```

### B. Update eval-orchestrator.md Agent

**Add new section after line 182 (in "Decision Tree")**:

```markdown
### User Intent: "Update dashboard" or "Ready to release"
**THIS IS THE MOST COMMON REQUEST - HANDLE IT FIRST!**

**Questions to ask:**
1. Have you run the baseline for this version?
2. Is this a dev baseline (3 models) or full (6 models)?
3. Do you want the dashboard to show this version only, or aggregate across all versions?

**Action - Standard Release Flow:**
```bash
# 1. Run baseline if not already done
make eval-baseline EVAL_VERSION=v0.3.12  # Or FULL=true for 6 models

# 2. Update dashboard with SPECIFIC version
ailang eval-report eval_results/baselines/v0.3.12 v0.3.12 --format=docusaurus > docs/docs/benchmarks/performance.md

# 3. Clear Docusaurus cache
cd docs && npm run clear

# 4. Restart dev server
cd docs && npm start

# 5. Verify at http://localhost:3000/ailang/docs/benchmarks/performance
```

**‚ö†Ô∏è CRITICAL**: DO NOT use `make benchmark-dashboard` for releases!
- It aggregates latest per-model across ALL baselines
- Will show mixed versions (confusing)
- Use `ailang eval-report <specific_dir>` instead

**Action - Multi-version Dashboard (for homepage)**:
```bash
# Only use this if user wants to show "best of each model"
make benchmark-dashboard
```

**Report back:**
- Version published
- Success rates (AILANG-only, not combined)
- Link to dashboard
- Any regressions found
```

### C. Add "Common Pitfalls" Section to eval-orchestrator.md

**Add at end of file (before "Success Criteria")**:

```markdown
## Common Pitfalls & How to Avoid Them

### Pitfall 1: Not Delegating to Agent
**Symptom**: User manually runs eval commands, dashboard doesn't update correctly
**Solution**: ALWAYS use eval-orchestrator agent for release workflows

### Pitfall 2: Using `make benchmark-dashboard` for Releases
**Symptom**: Dashboard shows v0.3.9 even though v0.3.12 baseline exists
**Cause**: Multi-model aggregation picks latest per-model, not latest version
**Solution**: Use `ailang eval-report <baseline_dir> <version>` instead

### Pitfall 3: Docusaurus Cache Not Cleared
**Symptom**: "Uncaught runtime errors" or webpack chunk 404s in browser
**Cause**: React components changed but webpack cache stale
**Solution**: `cd docs && npm run clear && npm start`

### Pitfall 4: Wrong JSON File Used
**Symptom**: Dashboard shows "null" for aggregates
**Cause**: Used performance matrix JSON instead of baseline results
**Solution**:
```bash
# ‚ùå WRONG
cp eval_results/performance_tables/v0.3.12.json docs/static/benchmarks/latest.json

# ‚úÖ CORRECT
ailang eval-report eval_results/baselines/v0.3.12 v0.3.12 --format=json > docs/static/benchmarks/latest.json
```

### Pitfall 5: Manually Editing Files
**Symptom**: JSON corruption, missing history, validation errors
**Cause**: Trying to manually copy/edit dashboard files
**Solution**: ALWAYS use `ailang eval-report` - it handles history, validation, atomic writes
```

### D. Update Makefile - Add Help Text

```makefile
.PHONY: help-release
help-release: ## Show release workflow (eval + dashboard)
	@echo "üì¶ RELEASE WORKFLOW"
	@echo ""
	@echo "1. Run baseline:"
	@echo "   make eval-baseline EVAL_VERSION=v0.3.X"
	@echo ""
	@echo "2. Update dashboard:"
	@echo "   ailang eval-report eval_results/baselines/v0.3.X v0.3.X --format=docusaurus > docs/docs/benchmarks/performance.md"
	@echo ""
	@echo "3. Restart docs:"
	@echo "   cd docs && npm run clear && npm start"
	@echo ""
	@echo "‚ö†Ô∏è  DO NOT use 'make benchmark-dashboard' for releases!"
	@echo "    It aggregates across versions (confusing)."
```

## Implementation Status

- [x] **Add "RELEASE WORKFLOW" section to CLAUDE.md** ‚úÖ Added at line 746
- [x] **Add "Update dashboard" decision tree to eval-orchestrator.md** ‚úÖ Added at line 184
- [x] **Add "Common Pitfalls" section to eval-orchestrator.md** ‚úÖ Added at line 501
- [x] **Add `help-release` target to Makefile** ‚úÖ Added at line 849
- [x] **Update .claude/commands/release.md** ‚úÖ Added step 13 "Update website dashboard"
- [ ] Test the updated workflow with v0.3.13 release (pending next release)
- [x] Document improvements in design_docs ‚úÖ This file!

## Additional Discovery (2025-10-17)

**The /release command was missing dashboard updates entirely!** This explains why it kept getting forgotten - it wasn't in the documented release workflow at all.

Fixed by adding comprehensive step 13 with:
- Exact `ailang eval-report` commands to run
- JSON verification with jq
- Docusaurus cache clearing steps
- Local testing verification
- **Critical warning** against `make benchmark-dashboard` (multi-model aggregation pitfall)
- Final verification checklist

## Success Metrics

Release workflow is successful when:
- [ ] User says "ready to release" ‚Üí Agent handles everything
- [ ] Dashboard updates in <2 minutes with clear instructions
- [ ] No manual JSON editing required
- [ ] No Docusaurus cache issues
- [ ] Dashboard shows correct version on first try

---

**Created**: 2025-10-17  
**Implemented**: 2025-10-17  
**Session**: Dashboard update struggle for v0.3.12  
**Impact**: Reduces release dashboard update time from 2+ hours ‚Üí <5 minutes

## Files Modified

1. **CLAUDE.md** - Added "üéØ RELEASE WORKFLOW" section (line 746-809)
2. **.claude/commands/release.md** - Added step 13 "Update website dashboard" (line 134-164)
3. **.claude/agents/eval-orchestrator.md** - Added "Update dashboard" decision tree (line 184-234) + Common Pitfalls (line 501-558)
4. **Makefile** - Added `help-release` target (line 849-871)

## Testing

To test these improvements on next release (v0.3.13):
```bash
# User says: "Ready to release v0.3.13"
# Expected: Dashboard updates automatically via /release command
# Verification: Dashboard shows v0.3.13, no manual intervention needed
```

## Lessons Learned

1. **Documentation gaps hurt every release** - Missing workflow steps cause repeated frustration
2. **Agent delegation must be enforced** - Strong forcing functions (priority in decision trees) needed
3. **Common pitfalls should be documented** - Same mistakes happen repeatedly without documentation
4. **Quick reference commands save hours** - `make help-release` provides instant guidance
