# M-DASH: Eval Dashboard Reliability

**Status:** In Progress
**Milestone:** M-DASH (Dashboard Reliability)
**Target Version:** v0.3.10
**Created:** 2025-10-16
**Estimated Effort:** 8-12 hours (1-2 days)

## Problem Statement

### The Incident (v0.3.9, Oct 15 2025)

**What broke:**
```bash
# User ran this command to update the dashboard:
ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=json > docs/static/benchmarks/latest.json

# Result: Historical data was DESTROYED
# Before: 5 versions in history (v0.3.9, v0.3.8, v0.3.7-1, v0.3.6-24, v0.3.6-24-mini)
# After:  2 versions in history (v0.3.9, v0.3.7-45)
# Lost: v0.3.8, v0.3.7-1, v0.3.6-24-mini
```

**Required workaround:**
- Wrote custom Python script to manually restore historical data
- ~20 lines of code to merge JSON files
- 4 manual steps to update dashboard
- **This is NOT acceptable for a release workflow**

### Root Cause

The `ExportBenchmarkJSON()` function **regenerates history from scratch** by scanning `eval_results/baselines/*`:

```go
// cmd/ailang/eval_tools.go:317-323
baselines, _ := eval_analysis.ListBaselines()
var history []*Baseline
for _, v := range baselines {
    if baseline, err := eval_analysis.LoadBaselineByVersion(v); err == nil {
        history = append(history, baseline)  // ❌ Only includes baselines found on disk!
    }
}
```

**The flaw:** History is **derived** from available baselines, not **preserved** from existing JSON.

**Why this breaks:**
1. Old baselines get deleted to save disk space
2. `eval-report` can't find them anymore
3. History shrinks on every run
4. Eventually we only have the latest version

### Multiple Sources of Truth

**Three places store benchmark data:**

1. **eval_results/baselines/vX.Y.Z/baseline.json**
   - Source: Created by `make eval-baseline`
   - Issues:
     - Uses `git describe` for version (e.g., "v0.3.7-46-g2cfa80a" instead of "v0.3.9")
     - Caches wrong `success_count` (20 vs actual 74)
     - Gets deleted to save disk space

2. **docs/static/benchmarks/latest.json**
   - Source: Generated by `ailang eval-report --format=json`
   - Issues:
     - History destroyed on regeneration
     - No validation before writing
     - No atomic writes (can be corrupted)
     - **Should be the source of truth but isn't**

3. **eval_results/performance_tables/vX.Y.Z.json** (optional)
   - Source: Generated by `ailang eval-matrix`
   - Issues:
     - Not consistently created
     - Not used by dashboard
     - Third source of truth that doesn't match others

**Design flaw:** No single source of truth. Data is scattered and inconsistent.

## Goals

### Primary Goals

1. **History preservation:** `eval-report --format=json` must preserve all existing history
2. **Single source of truth:** `docs/static/benchmarks/latest.json` is the authoritative source
3. **Append-only:** New versions added to history, never removed
4. **Idempotent:** Running twice with same version updates entry, doesn't duplicate

### Secondary Goals

5. **Validation:** Never write corrupted JSON files
6. **Atomic writes:** No partial writes (all-or-nothing)
7. **Correct metadata:** baseline.json has explicit version (not git describe)
8. **Automated workflow:** No manual Python scripts

## Non-Goals

- **NOT** retroactively fixing historical baseline.json files (leave as-is for git history)
- **NOT** backfilling lost versions (v0.3.8, etc. are gone)
- **NOT** changing the dashboard UI (frontend already works)
- **NOT** archiving old baselines automatically (user's responsibility)

## Solution Design

### Milestone 1: History Preservation (CRITICAL)

**Goal:** Never destroy historical data

**Changes:**

1. **Read existing JSON before writing:**
```go
// internal/eval_analysis/export_docusaurus.go
func loadExistingDashboard(path string) (*DashboardJSON, error) {
    data, err := os.ReadFile(path)
    if os.IsNotExist(err) {
        return &DashboardJSON{History: []HistoryEntry{}}, nil
    }
    if err != nil {
        return nil, err
    }

    var dashboard DashboardJSON
    if err := json.Unmarshal(data, &dashboard); err != nil {
        return nil, fmt.Errorf("invalid JSON: %w", err)
    }
    return &dashboard, nil
}
```

2. **Merge history (append or update):**
```go
func mergeHistory(existing *DashboardJSON, newEntry HistoryEntry) {
    // Check for duplicate version
    for i, entry := range existing.History {
        if entry.Version == newEntry.Version {
            existing.History[i] = newEntry  // Update existing entry
            return
        }
    }
    // Prepend new entry (reverse chronological order)
    existing.History = append([]HistoryEntry{newEntry}, existing.History...)
}
```

3. **Update ExportBenchmarkJSON signature:**
```go
// Before: func ExportBenchmarkJSON(matrix, history, results) (string, error)
// After:  func ExportBenchmarkJSON(matrix, history, results, outputPath) (string, error)

func ExportBenchmarkJSON(matrix *PerformanceMatrix, history []*Baseline,
                         results []*BenchmarkResult, outputPath string) (string, error) {
    // Load existing dashboard (preserves history)
    existing, err := loadExistingDashboard(outputPath)
    if err != nil {
        return "", fmt.Errorf("failed to load existing dashboard: %w", err)
    }

    // Build new entry from matrix
    newEntry := buildHistoryEntryFromMatrix(matrix, results)

    // Merge with existing history
    mergeHistory(existing, newEntry)

    // Update current version data
    existing.Version = matrix.Version
    existing.Timestamp = time.Now().Format(time.RFC3339)
    existing.Aggregates = matrix.Aggregates
    existing.Models = matrix.Models
    existing.Benchmarks = matrix.Benchmarks

    // Marshal and return
    jsonBytes, err := json.MarshalIndent(existing, "", "  ")
    return string(jsonBytes), err
}
```

4. **Wire to CLI:**
```go
// cmd/ailang/eval_tools.go:339
output, err := eval_analysis.ExportBenchmarkJSON(matrix, history, results,
                                                   "docs/static/benchmarks/latest.json")
```

**LOC estimate:** ~150 LOC implementation + ~100 LOC tests = ~250 LOC
**Time estimate:** 4-6 hours

---

### Milestone 2: Validation & Atomic Writes (HIGH)

**Goal:** Never write corrupted files

**Changes:**

1. **Validation:**
```go
// internal/eval_analysis/types.go
type DashboardJSON struct {
    Version    string                    `json:"version"`
    Timestamp  string                    `json:"timestamp"`
    TotalRuns  int                       `json:"totalRuns"`
    Aggregates map[string]interface{}    `json:"aggregates"`
    Models     map[string]interface{}    `json:"models"`
    Benchmarks map[string]interface{}    `json:"benchmarks"`
    Languages  []string                  `json:"languages"`
    History    []HistoryEntry            `json:"history"`
}

func (d *DashboardJSON) Validate() error {
    if d.Version == "" {
        return errors.New("version required")
    }
    if d.Timestamp == "" {
        return errors.New("timestamp required")
    }
    if len(d.History) == 0 {
        return errors.New("history must have at least one entry")
    }

    // Check for duplicate versions in history
    seen := make(map[string]bool)
    for _, entry := range d.History {
        if seen[entry.Version] {
            return fmt.Errorf("duplicate version in history: %s", entry.Version)
        }
        seen[entry.Version] = true
    }

    return nil
}
```

2. **Atomic writes:**
```go
// internal/eval_analysis/export_docusaurus.go
func writeJSONAtomic(path string, data interface{}) error {
    jsonBytes, err := json.MarshalIndent(data, "", "  ")
    if err != nil {
        return fmt.Errorf("failed to marshal JSON: %w", err)
    }

    tmpPath := path + ".tmp"

    // Write to temp file
    if err := os.WriteFile(tmpPath, jsonBytes, 0644); err != nil {
        return fmt.Errorf("failed to write temp file: %w", err)
    }

    // Validate temp file
    tmpData, err := os.ReadFile(tmpPath)
    if err != nil {
        os.Remove(tmpPath)
        return err
    }

    var test DashboardJSON
    if err := json.Unmarshal(tmpData, &test); err != nil {
        os.Remove(tmpPath)
        return fmt.Errorf("validation failed: %w", err)
    }

    if err := test.Validate(); err != nil {
        os.Remove(tmpPath)
        return fmt.Errorf("validation failed: %w", err)
    }

    // Atomic rename (on Unix, overwrites atomically)
    if err := os.Rename(tmpPath, path); err != nil {
        os.Remove(tmpPath)
        return fmt.Errorf("failed to rename: %w", err)
    }

    return nil
}
```

3. **Integration:**
```go
// Update ExportBenchmarkJSON to use atomic writes
func ExportBenchmarkJSON(...) (string, error) {
    // ... build dashboard ...

    // Write atomically
    if err := writeJSONAtomic(outputPath, dashboard); err != nil {
        return "", err
    }

    // Return JSON string for stdout (backwards compat)
    jsonBytes, _ := json.MarshalIndent(dashboard, "", "  ")
    return string(jsonBytes), nil
}
```

**LOC estimate:** ~80 LOC implementation + ~50 LOC tests = ~130 LOC
**Time estimate:** 2-3 hours

---

### Milestone 3: Baseline Metadata Fixes (MEDIUM)

**Goal:** baseline.json has correct version and stats

**Changes:**

1. **Fix tools/eval_baseline.sh:**
```bash
#!/bin/bash
set -e

# Before: VERSION="${1:-$(git describe --tags --always)}"
# After:  Require explicit version

VERSION="${1}"
if [ -z "$VERSION" ]; then
    echo "Error: Version is required"
    echo "Usage: make eval-baseline VERSION=v0.3.10"
    exit 1
fi

GIT_DESCRIBE="$(git describe --tags --always 2>/dev/null || echo "unknown")"
GIT_COMMIT="$(git rev-parse HEAD 2>/dev/null || echo "unknown")"
GIT_BRANCH="$(git branch --show-current 2>/dev/null || echo "unknown")"
GIT_DIRTY="false"
if ! git diff-index --quiet HEAD -- 2>/dev/null; then
    GIT_DIRTY="true"
fi

# ... rest of script ...

# baseline.json format (updated):
cat > "$BASELINE_DIR/baseline.json" <<EOF
{
  "version": "$VERSION",
  "git_describe": "$GIT_DESCRIBE",
  "git_commit": "$GIT_COMMIT",
  "git_branch": "$GIT_BRANCH",
  "git_dirty": $GIT_DIRTY,
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "models": "$MODELS",
  "full_suite": $FULL_SUITE,
  "languages": "$LANGUAGES",
  "parallel": $PARALLEL,
  "total_runs": $TOTAL_RUNS
}
EOF
# Note: Removed success_count (calculate from result files instead)
```

2. **Update Makefile:**
```makefile
.PHONY: eval-baseline
eval-baseline: build
	@if [ -z "$(VERSION)" ]; then \
		echo "Error: VERSION parameter required"; \
		echo "Usage: make eval-baseline VERSION=v0.3.10"; \
		exit 1; \
	fi
	@VERSION=$(VERSION) ./tools/eval_baseline.sh
```

3. **Update LoadBaselineByVersion:**
```go
// internal/eval_analysis/loader.go
// Remove calculation of SuccessCount from baseline.json
// Always calculate from result files:
func LoadBaselineByVersion(version string) (*Baseline, error) {
    // ... load baseline.json ...

    // Calculate success count from results (don't trust cached value)
    successCount := 0
    for _, result := range results {
        if result.StdoutOk {
            successCount++
        }
    }
    baseline.SuccessCount = successCount

    return baseline, nil
}
```

**LOC estimate:** ~50 LOC changes + documentation
**Time estimate:** 1-2 hours

---

## Testing Strategy

### Unit Tests (New)

**test: TestHistoryPreservation**
```go
func TestHistoryPreservation(t *testing.T) {
    // Create temp file with existing history (2 versions)
    existing := `{"version":"v0.3.9","history":[
        {"version":"v0.3.9","timestamp":"..."},
        {"version":"v0.3.8","timestamp":"..."}
    ]}`
    tmpFile := writeTempJSON(t, existing)

    // Create new matrix for v0.3.10
    matrix := &PerformanceMatrix{Version: "v0.3.10"}

    // Export (should preserve history)
    _, err := ExportBenchmarkJSON(matrix, nil, nil, tmpFile)
    assert.NoError(t, err)

    // Read result
    result := readJSON(t, tmpFile)

    // Verify: 3 versions now (v0.3.10, v0.3.9, v0.3.8)
    assert.Len(t, result.History, 3)
    assert.Equal(t, "v0.3.10", result.History[0].Version)
    assert.Equal(t, "v0.3.9", result.History[1].Version)
    assert.Equal(t, "v0.3.8", result.History[2].Version)
}
```

**test: TestDuplicateVersionUpdate**
```go
func TestDuplicateVersionUpdate(t *testing.T) {
    // Existing history has v0.3.9
    existing := `{"version":"v0.3.9","history":[
        {"version":"v0.3.9","successRate":0.50}
    ]}`
    tmpFile := writeTempJSON(t, existing)

    // Export v0.3.9 again (rerun with updated data)
    matrix := &PerformanceMatrix{
        Version: "v0.3.9",
        Aggregates: Aggregates{FinalSuccess: 0.60},
    }
    _, err := ExportBenchmarkJSON(matrix, nil, nil, tmpFile)
    assert.NoError(t, err)

    // Read result
    result := readJSON(t, tmpFile)

    // Verify: Still 1 entry, but updated
    assert.Len(t, result.History, 1)
    assert.Equal(t, 0.60, result.History[0].SuccessRate)
}
```

**test: TestValidation**
```go
func TestValidationCatchesMissingVersion(t *testing.T) {
    d := &DashboardJSON{History: []HistoryEntry{}}
    err := d.Validate()
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "version required")
}

func TestValidationCatchesDuplicates(t *testing.T) {
    d := &DashboardJSON{
        Version: "v0.3.9",
        History: []HistoryEntry{
            {Version: "v0.3.9"},
            {Version: "v0.3.9"},  // Duplicate!
        },
    }
    err := d.Validate()
    assert.Error(t, err)
    assert.Contains(t, err.Error(), "duplicate version")
}
```

**test: TestAtomicWrites**
```go
func TestAtomicWriteRollsBackOnError(t *testing.T) {
    tmpPath := "/tmp/test_dashboard.json"

    // Write valid initial data
    initial := &DashboardJSON{Version: "v0.3.9", History: []HistoryEntry{{Version: "v0.3.9"}}}
    writeJSONAtomic(tmpPath, initial)

    // Try to write invalid data
    invalid := &DashboardJSON{Version: "", History: []HistoryEntry{}}  // Missing version
    err := writeJSONAtomic(tmpPath, invalid)
    assert.Error(t, err)

    // Verify: Original file unchanged
    data, _ := os.ReadFile(tmpPath)
    var result DashboardJSON
    json.Unmarshal(data, &result)
    assert.Equal(t, "v0.3.9", result.Version)  // Still has old data
}
```

### Integration Tests (Manual)

**Test 1: History preservation**
```bash
# Backup current dashboard
cp docs/static/benchmarks/latest.json /tmp/backup_v1.json

# Check current history count
BEFORE=$(jq '.history | length' /tmp/backup_v1.json)
echo "Before: $BEFORE versions"

# Run report (should preserve history)
ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=json --output docs/static/benchmarks/latest.json

# Check new history count
AFTER=$(jq '.history | length' docs/static/benchmarks/latest.json)
echo "After: $AFTER versions"

# Verify: AFTER >= BEFORE (never shrinks)
if [ "$AFTER" -lt "$BEFORE" ]; then
    echo "❌ FAILED: History count decreased!"
    exit 1
fi
echo "✅ PASSED: History preserved"
```

**Test 2: Idempotency**
```bash
# Run twice with same version
ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=json --output /tmp/test_v1.json

ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=json --output /tmp/test_v1.json

# Count v0.3.9 occurrences in history
COUNT=$(jq '[.history[] | select(.version == "v0.3.9")] | length' /tmp/test_v1.json)

if [ "$COUNT" -ne 1 ]; then
    echo "❌ FAILED: Duplicate version entries!"
    exit 1
fi
echo "✅ PASSED: Idempotent (no duplicates)"
```

**Test 3: Atomic writes**
```bash
# Simulate crash during write
function crash_during_write() {
    sleep 0.5 && killall -9 ailang &
    ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
      --format=json --output /tmp/crash_test.json
}

crash_during_write

# Verify: File is either old content OR new content (never partial)
jq '.' /tmp/crash_test.json > /dev/null
if [ $? -eq 0 ]; then
    echo "✅ PASSED: JSON is valid after crash"
else
    echo "❌ FAILED: Corrupted JSON after crash"
fi
```

### Regression Prevention (CI)

**Add to .github/workflows/ci.yml:**
```yaml
- name: Verify dashboard history never shrinks
  run: |
    BEFORE=$(jq '.history | length' docs/static/benchmarks/latest.json)

    # Regenerate dashboard
    make eval-baseline VERSION=test-ci
    ailang eval-report eval_results/baselines/test-ci test-ci \
      --format=json --output docs/static/benchmarks/latest.json

    AFTER=$(jq '.history | length' docs/static/benchmarks/latest.json)

    if [ "$AFTER" -lt "$BEFORE" ]; then
      echo "Error: History count decreased from $BEFORE to $AFTER"
      exit 1
    fi
```

---

## Updated Workflow (v0.3.10+)

### Before (v0.3.9 and earlier)

❌ **4 manual steps, prone to data loss:**

```bash
# 1. Create baseline
make eval-baseline VERSION=v0.3.9

# 2. Generate report (DESTROYS HISTORY!)
ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=json > docs/static/benchmarks/latest.json

# 3. Manually restore history with Python script
python tools/fix_dashboard_history.py

# 4. Update Docusaurus page
ailang eval-report eval_results/baselines/v0.3.9 v0.3.9 \
  --format=docusaurus > docs/docs/benchmarks/performance.md
```

### After (v0.3.10+)

✅ **2 automated steps, history preserved:**

```bash
# 1. Create baseline with explicit version
make eval-baseline VERSION=v0.3.10

# 2. Update dashboard and docs (ONE command, preserves history)
ailang eval-report eval_results/baselines/v0.3.10 v0.3.10 \
  --format=json --output docs/static/benchmarks/latest.json && \
ailang eval-report eval_results/baselines/v0.3.10 v0.3.10 \
  --format=docusaurus > docs/docs/benchmarks/performance.md
```

**Benefits:**
- History automatically preserved
- No manual Python scripts
- Idempotent (safe to rerun)
- Atomic writes (no corruption)
- Validated before writing

---

## Rollout Plan

### Phase 1: Implementation (Day 1-2)

1. Create feature branch: `fix/eval-dashboard-reliability`
2. Implement M1: History preservation (~4-6h)
3. Add unit tests (~2h)
4. Test with v0.3.9 data manually
5. Commit and push

### Phase 2: Validation (Day 2)

6. Implement M2: Validation and atomic writes (~2-3h)
7. Add unit tests (~1h)
8. Implement M3: Baseline metadata fixes (~1-2h)

### Phase 3: Documentation (Day 2)

9. Update CLAUDE.md with new workflow
10. Add regression tests to CI
11. Create PR with detailed description

### Phase 4: Verification (Before merge)

12. Run full integration test suite
13. Backup current dashboard
14. Test with actual v0.3.9 baseline
15. Verify no history loss
16. Merge to dev

---

## Success Metrics

### Before (v0.3.9 and earlier)

| Metric | Status |
|--------|--------|
| History preservation | ❌ Lost 3 versions (v0.3.8, v0.3.7-1, v0.3.6-24-mini) |
| Manual steps | ❌ 4 steps (create baseline, generate report, fix history, update docs) |
| Python scripts | ❌ Required custom script (~20 lines) |
| Validation | ❌ None (JSON can be corrupted) |
| Atomic writes | ❌ Direct writes (partial writes possible) |
| Idempotency | ❌ Running twice creates duplicates |
| baseline.json version | ❌ Uses git describe ("v0.3.7-46-g2cfa80a") |
| baseline.json stats | ❌ Cached wrong success_count (20 vs 74) |

### After (v0.3.10+)

| Metric | Target |
|--------|--------|
| History preservation | ✅ All versions preserved |
| Manual steps | ✅ 2 steps (create baseline, update dashboard) |
| Python scripts | ✅ None (fully automated) |
| Validation | ✅ JSON validated before writing |
| Atomic writes | ✅ Temp file + atomic rename |
| Idempotency | ✅ Updates entry, no duplicates |
| baseline.json version | ✅ Explicit version ("v0.3.10") |
| baseline.json stats | ✅ Calculated from result files |

---

## Implementation Checklist

### M1: History Preservation (4-6h)
- [ ] Add `DashboardJSON` type to types.go
- [ ] Implement `loadExistingDashboard()`
- [ ] Implement `mergeHistory()`
- [ ] Implement `buildHistoryEntryFromMatrix()`
- [ ] Update `ExportBenchmarkJSON()` signature
- [ ] Wire to CLI in eval_tools.go
- [ ] Add `TestHistoryPreservation()`
- [ ] Add `TestDuplicateVersionUpdate()`
- [ ] Test with v0.3.9 data manually

### M2: Validation & Atomic Writes (2-3h)
- [ ] Add `DashboardJSON.Validate()` method
- [ ] Implement `writeJSONAtomic()`
- [ ] Integrate validation into export path
- [ ] Add `TestValidationCatchesMissingVersion()`
- [ ] Add `TestValidationCatchesDuplicates()`
- [ ] Add `TestAtomicWriteRollsBackOnError()`

### M3: Baseline Metadata Fixes (1-2h)
- [ ] Update tools/eval_baseline.sh (require VERSION)
- [ ] Update Makefile (require VERSION parameter)
- [ ] Update baseline.json format (separate version and git_describe)
- [ ] Remove success_count from baseline.json (calculate dynamically)
- [ ] Update LoadBaselineByVersion() to calculate stats

### Documentation
- [ ] Update CLAUDE.md with new workflow
- [ ] Add regression tests to CI
- [ ] Create detailed PR description
- [ ] Document breaking changes (require VERSION)

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Break existing dashboards | Low | High | Test with v0.3.9 data before merge |
| Corrupt JSON files | Low | High | Atomic writes + validation |
| Lost history during rollout | Low | High | Backup latest.json before deploying |
| Users run old command | Medium | Medium | Add deprecation warning to old workflow |
| Tests don't cover edge cases | Medium | Low | Manual integration testing |
| Version confusion | Low | Low | Clear error messages, updated docs |

**Overall risk:** **Low** (well-scoped problem, clear solution, good test coverage)

---

## Future Enhancements (Out of scope)

- **Auto-archive:** Move old baselines to archive/ after dashboard update
- **Diffing:** Show what changed between versions in CLI output
- **Web UI:** Dashboard page to browse historical data
- **Metrics tracking:** Track dashboard update frequency, size growth
- **Compression:** Compress old history entries to save space

---

## References

- **Original design doc:** `design_docs/planned/eval-dashboard-reliability.md`
- **Issue incident:** Commit 0cc0d89 (manual workaround for v0.3.9)
- **Code locations:**
  - `internal/eval_analysis/export_docusaurus.go` - JSON export logic
  - `cmd/ailang/eval_tools.go` - CLI command handler
  - `tools/eval_baseline.sh` - Baseline creation script
  - `docs/static/benchmarks/latest.json` - Dashboard data file

---

## Appendix: File Structure

```
eval_results/
├── baselines/
│   ├── v0.3.9/
│   │   ├── baseline.json          # Metadata (version, models, etc.)
│   │   ├── <id>_<lang>_<model>.json  # Individual results (126 files)
│   │   └── summary.jsonl          # Optional JSONL export
│   └── v0.3.10/
│       └── ...
└── performance_tables/
    ├── v0.3.9.json                # Optional matrix export
    └── v0.3.10.json

docs/
├── static/
│   └── benchmarks/
│       └── latest.json            # ⭐ SINGLE SOURCE OF TRUTH (dashboard data)
└── docs/
    └── benchmarks/
        └── performance.md         # Docusaurus page (markdown)
```

**Key insight:** Only `latest.json` needs to preserve history. Individual baselines can be deleted after dashboard update.
