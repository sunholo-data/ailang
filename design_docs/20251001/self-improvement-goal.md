ðŸ“„ AILANG Design Doc: Self-Improving Programs via Tight Feedback Cycles

Author: AILANG Working Group
Date: October 2025
Version: Draft v0.1

â¸»

1. Vision

AILANG should become the default language for self-improving software, where the feedback loop between proposing changes, evaluating them deterministically, and persisting improvements is first-class in the language runtime and type system.

Tagline: Programs that evolve faster and safer than in any other language.

â¸»

2. Motivation

Traditional languages (Python, Go, Rust, etc.) can orchestrate self-improvement loops, but:
	â€¢	They lack built-in safety: resource exhaustion, uncontrolled network calls, runaway costs, and non-deterministic execution are pervasive.
	â€¢	They lack determinism: evaluation results vary due to hidden state, random seeds, or external data drift.
	â€¢	They lack observability & provenance: improvements aren't checkpointed, reproducible, or auditable.
	â€¢	They lack integration with AI APIs: model calls, token budgets, and retries are bolted on via third-party libs.

AILANGâ€™s advantage: effects, budgets, and reproducibility are part of the language core. That makes it uniquely suited for self-improving systems, from local deterministic loops to distributed AI agents.

â¸»

3. Design Goals

Primary
	1.	Tight Feedback Cycles
	â€¢	Run: propose â†’ evaluate â†’ checkpoint â†’ repeat.
	â€¢	Low overhead, predictable, hermetic.
	2.	Safety by Construction
	â€¢	Effects (! {AI, Net, FS, Rand, Clock, DB, Trace}) must be declared.
	â€¢	Resource budgets must be explicit.
	â€¢	Deterministic evaluation by default.
	â€¢	NOTE: AI effect introduced in v0.2, joins 8 existing canonical effects.
	3.	Reproducibility & Provenance
	â€¢	Content-addressed artifacts (SHA256 digests).
	â€¢	Seeds, datasets, configs all pinned.
	â€¢	Ledger/trace persisted automatically.
	4.	Composable Optimizers
	â€¢	Hill climbing, bandits, evolutionary search as library functions.
	â€¢	Evaluators are just user-written pure functions.

Secondary
	â€¢	Human-in-the-loop gates (PRs, approvals, rollbacks).
	â€¢	Support both local evaluation and remote AI model calls.
	â€¢	Progressive disclosure: beginners can run small local loops, experts can orchestrate complex multi-agent experiments.

3.5 Anti-Goals

What AILANG explicitly does NOT do:
	1.	Auto-apply improvements - Always require human review/approval before production deployment
	2.	Unbounded optimization - Every loop must have explicit iteration/budget limits
	3.	Opaque AI calls - All model interactions logged, reproducible via seed/digest
	4.	Mutable checkpoints - Artifacts are content-addressed and immutable
	5.	Implicit resource usage - No silent network calls, file writes, or token consumption

Why: Self-improving systems must be safe by default, transparent always, and controllable explicitly.

â¸»

4. Core Language Features to Enable Self-Improvement

	4.1	Effects + Budgets

	Budget syntax (row-polymorphic, composable):

-- v0.2: Row-polymorphic budgets
func callModel[e](p: Prompt) -> Result[Text, Error]
  ! {AI | e}
  with budget { tokens: 50_000, requests: 5, timeout: 3.s }

-- v0.3: Budget as capability (more testable)
type AIBudget = { tokens: Int, requests: Int, timeout: Duration }

func callModel(p: Prompt, budget: AIBudget) -> Result[Text, Error]
  ! {AI with budget}

	â€¢	Compile-time check: functions must declare when they use AI/Net/FS/etc.
	â€¢	Runtime check: budgets enforced automatically (fail with BudgetExceeded).
	â€¢	Design: Budgets as capabilities (passed explicitly) are more testable and composable.

	4.2	Checkpoints / Artifact Store

	Built-in module std/checkpoint with deterministic digests:

module std/checkpoint

-- Save returns content-addressed digest (SHA256)
export func save[a](data: a) -> Digest ! {FS}

-- Load is pure given a digest (immutable store)
export pure func load(digest: Digest) -> Result[Blob, NotFound] ! {FS}

-- Garbage collection (explicit, not automatic)
export func prune(before: Timestamp, keep: [Digest]) -> () ! {FS}

-- Provenance: link digest to source code SID
export func tag(digest: Digest, meta: Metadata) -> () ! {FS}

	â€¢	Everything self-improvement loops touch can be persisted/reproduced.
	â€¢	Hash collisions: Use SHA256, astronomically unlikely; fail loudly on collision.
	â€¢	GC strategy: Explicit prune() calls, no automatic cleanup.

	4.3	Ledger / Trace

	Append-only log of runs with queryable schema:

-- Ledger: append-only log of runs
type LedgerEntry = {
  runId: Digest,
  timestamp: Timestamp,
  seed: Int,
  effects: EffectRow,
  budgets: BudgetMap,
  artifacts: [Digest],
  result: Result[Digest, Error],
  trace: [TraceEvent]  -- for debugging
}

export func append(entry: LedgerEntry) -> () ! {FS}
export func query(predicate: LedgerEntry -> Bool) -> [LedgerEntry] ! {FS}

	â€¢	Every run records effects, budgets, artifacts, seeds.
	â€¢	Deterministic runs can be diffed or replayed.
	â€¢	Enables "what changed?" analysis between runs.

	4.4	Optimizers & Evaluators as First-Class Citizens
	â€¢	std/opt.hillClimb, std/opt.bandit, std/opt.evolutionary.
	â€¢	std/eval.scoreDataset, std/eval.assertGolden.

	4.5	Deterministic Seeds
	â€¢	Every run seeded at initialization (via CLI or config), recorded in ledger with full provenance.
	â€¢	Guarantees reproducible results unless Rand or Clock effects are allowed.

â¸»

5. Example Workflow

Prompt Optimization Loop (Enhanced with Error Handling & Checkpointing)

import std/opt (hillClimb, Config)
import std/eval (scoreDataset)
import std/checkpoint (save, load)
import std/io (println)

type Candidate = { prompt: String, digest: Option[Digest] }

-- Evaluator: pure, deterministic, cacheable
pure func evaluate(c: Candidate, ds: Dataset) -> Float {
  scoreDataset(ds, c.prompt)
}

-- Proposer: uses AI, handles errors, respects budget
func propose(c: Candidate) -> Result[Candidate, AIError]
  ! {AI, Net}
  with budget { tokens: 1000, requests: 1, timeout: 5.s }
{
  match callModel("improve this prompt", c.prompt) {
    Ok(suggestion) => Ok({ prompt: suggestion, digest: None }),
    Err(e) => Err(e)
  }
}

-- Pure mutator: always succeeds
pure func mutate(c: Candidate) -> [Candidate] {
  [ { prompt: c.prompt ++ "\nBe concise.", digest: None }
  , { prompt: c.prompt ++ "\nCite sources.", digest: None }
  ]
}

-- Main loop: orchestrates, checkpoints, handles failures
func main(ds: Dataset, seed: Candidate, iters: Int)
  -> Result[Candidate, Error] ! {AI, Net, FS}
  with budget { tokens: 50_000, requests: 20, wall: 60.s }
{
  let config = Config {
    maxIters: iters,
    patience: 5,  -- early stopping
    minimize: false
  };

  hillClimb(
    seed,
    \c. evaluate(c, ds),
    \c. match propose(c) {
      Ok(new) => mutate(c) ++ [new],  -- mix AI + heuristics
      Err(_) => mutate(c)              -- fallback on AI failure
    },
    config
  ) |> save  -- checkpoint final result
}

Properties:
	â€¢	Effects explicit: {AI, Net, FS}.
	â€¢	Budgets enforced for tokens, requests, and wall time.
	â€¢	Error handling: AI failures fall back to pure heuristics.
	â€¢	Early stopping: patience parameter prevents overfitting.
	â€¢	Checkpointing: Final result saved to immutable artifact store.
	â€¢	Ledger records every candidate, score, and digest.
	â€¢	Improvement loop is safe, reproducible, and resilient.

â¸»

6. Phased Roadmap

v0.1 (Current MVP - October 2025)
	â€¢	Pure deterministic evaluation loops (COMPLETE âœ…)
	â€¢	Module system for composable optimizers (COMPLETE âœ…)
	â€¢	Effect system foundation (COMPLETE âœ…)
	â€¢	Checkpoints as FS digests (TODO for v0.2)
	â€¢	Golden test integration (TODO for v0.2)
	â€¢	No AI/Net yet â€” use offline heuristics.

v0.2 (~2-3 weeks after v0.1.0 ships)
	- [ ] Add `AI` to CanonicalEffects (1 day)
	- [ ] Implement `std/checkpoint` with SHA256 digests (3 days)
	- [ ] Add budget syntax to effect parser (2 days)
	- [ ] Runtime budget enforcement (tokens, requests, wall time) (4 days)
	- [ ] `std/opt.hillClimb` with early stopping (3 days)
	- [ ] Ledger append/query API (2 days)
	- [ ] Example: prompt optimizer with GPT-4 (1 day)
	- [ ] Integration tests: budget exhaustion, checkpoint recovery (2 days)

	Success Criteria:
	- âœ… Can run 100-iteration hillClimb loop with AI effect
	- âœ… Budget exceeded â†’ graceful failure with partial results
	- âœ… Checkpoints are byte-for-byte reproducible
	- âœ… Ledger queryable for "which run had best score?"

v0.3
	â€¢	Bandits & evolutionary optimizers.
	â€¢	Auto-PR + rollback modules.
	â€¢	Canary runs.
	â€¢	Basic human-in-loop review hooks.
	â€¢	Effect handlers (intercept AI calls for caching, mocking, rate limiting).

v0.4
	â€¢	Service mode (ailang serve) â†’ schedule improvement jobs.
	â€¢	Signed artifacts, provenance receipts.
	â€¢	Multi-agent negotiation of interfaces.
	â€¢	Row polymorphism for "this optimizer works with ANY effects".

â¸»

7. Applications
	â€¢	Prompt tuning & fine-tuning search (LLM ops).
	â€¢	Auto-ETL optimization: improving data cleaning scripts deterministically.
	â€¢	Config optimizers: hyperparams, feature flags.
	â€¢	Agent skill learning: chaining AI API calls with resource guards.
	â€¢	Self-upgrading libraries: stdlib functions propose & verify their own improvements.

â¸»

7.5 Security & Safety

Self-improving systems pose unique risks. AILANG mitigates them:

1. Prompt Injection Attacks
   â€¢	Mitigation: Sandboxed evaluators, input validation at type level
   â€¢	Example: `Prompt` newtype that sanitizes on construction

2. Resource Exhaustion
   â€¢	Mitigation: Budgets enforced at runtime, not advisory
   â€¢	Example: Loop that hits token limit fails fast, doesn't retry

3. Data Poisoning
   â€¢	Mitigation: Dataset digests pinned, immutable checkpoint store
   â€¢	Example: Training on tampered data â†’ digest mismatch â†’ rejection

4. Unintended Optimization
   â€¢	Mitigation: Pure evaluators, human review before production
   â€¢	Example: Optimizer finds adversarial prompt â†’ caught in review

5. Effect Leakage
   â€¢	Mitigation: Type system prevents undeclared effects
   â€¢	Example: "Pure" evaluator can't make network calls

Design Principle: Fail closed, not open. Budget exhaustion, missing artifacts, or type errors should halt execution, not continue with degraded behavior.

â¸»

8. Open Questions
	â€¢	Should artifact store be local-only (FS digests) in v0.1, or integrate with cloud (S3/GCS) early?
	â€¢	How much of budget enforcement should be compile-time vs runtime?
	â€¢	Should optimizers themselves declare effects (Rand, Clock)?
	â€¢	What governance do we enforce around auto-PRs (thresholds, approvals)?

â¸»

9. Success Criteria
	â€¢	v0.1: run local optimization loops deterministically, reproducible by digest.
	â€¢	v0.2: safe AI/Net calls with enforced budgets.
	â€¢	v0.3: non-trivial programs (prompt optimizer, config tuner, ETL improver) run stably.
	â€¢	v0.4: AILANG programs evolve themselves faster than any human-written workflow.

â¸»

10. Competitive Positioning

Why AILANG is uniquely suited for self-improving software:

vs Python
	â€¢	Type safety, determinism, budget enforcement at language level
	â€¢	No silent failures, no hidden state, no runaway costs

vs Rust
	â€¢	Higher-level abstractions for AI workflows
	â€¢	Effect system makes resource control ergonomic, not burdensome

vs Haskell
	â€¢	Simpler effect system (rows, not free monads)
	â€¢	Better AI library ecosystem (future)
	â€¢	Pragmatic defaults (determinism without ceremony)

vs LangChain/AutoGPT/CrewAI
	â€¢	Provable safety properties via type system
	â€¢	Reproducibility via content-addressed artifacts
	â€¢	Budget enforcement prevents runaway costs
	â€¢	Pure evaluators prevent non-deterministic drift

Tagline: "The language for self-improving software that's safe, reproducible, and provably correct."

No other language can credibly make this claim.

â¸»

ðŸ‘‰ This design doc deliberately keeps self-improvement as the north star. Every new feature (effects, budgets, checkpoints, optimizers) should be judged by: does it make the feedback cycle safer, faster, or more reproducible?

Implementation Status (Oct 2025): v0.1.0 complete with effect system, imports, stdlib foundation. v0.2 roadmap above provides concrete path to AI effect + budgets + checkpointing.
